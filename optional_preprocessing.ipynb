{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this every time you open the spreadsheet\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import Counter\n",
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "# This function returns tweets and test_tweets, both lists of tweets\n",
    "tweets, test_tweets = lib.read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous notebooks, we have implemented a Naive Bayes classifier on the data. Let's remind of ourselves about how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Energy', 'Food', 'Medical', 'Water', 'None']\n",
    "\n",
    "probs = {}\n",
    "for category in categories:\n",
    "    prior_prob, token_prob = lib.calc_probs_single(tweets, category)\n",
    "    probs[category] = (prior_prob, token_prob)\n",
    "\n",
    "# Get average F1 score for the test set\n",
    "predictions = [(tweet, lib.classify_nb_single(tweet, probs)) for tweet in test_tweets] # maps each test tweet to its predicted label\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good, right! :) We would like to furthur enhance the performance though. Some questions you may have:\n",
    "1. Are all words equally informative?\n",
    "2. Words such as \"*generator*\" and \"*generators*\" seem to convey the same meaning. Can we merge them?\n",
    "\n",
    "Next, we are going to play with three pre-processing steps to address these two questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Stop words, or function words (as opposed to *content words*), refer to commonly used words that are usually non-informative, such as \"*the*\", \"*a*\", or \"*can*\".\n",
    "\n",
    "It is usually advantageous for the classifier to ignore these stop words, since they may add noises or cause numerical issues (e.g. underflow).\n",
    "\n",
    "The `nltk` package provides a list of stop words in English, and we can remove them from our data simply by using equality tests, which can be considered as a *rule-based classifier* that classifies whether a word is a stop word or not by looking up a blacklist (i.e. the list of stop words).\n",
    "\n",
    "Let's first look at some examples of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "# look at some stopwords\n",
    "print(\"Here are some example stopwords:\")\n",
    "for i,word in enumerate(eng_stopwords):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of filtering a tweet using the stop word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]\n",
    "tokens = tweet.tokenSet\n",
    "print('all tokens:\\n', tokens, '\\n')\n",
    "\n",
    "filtered_tokens = set()\n",
    "deleted_tokens = set()\n",
    "\n",
    "for token in tweet.tokenSet:\n",
    "    if token in eng_stopwords:\n",
    "        deleted_tokens.add(token)\n",
    "    else:\n",
    "        filtered_tokens.add(token)\n",
    "\n",
    "print('filtered_tokens:\\n', filtered_tokens, '\\n')\n",
    "print('deleted_tokens:\\n', deleted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's see if removing stop words actually helps with the classification performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Energy', 'Food', 'Medical', 'Water', 'None']\n",
    "\n",
    "probs = {}\n",
    "for category in categories:\n",
    "    prior_prob, token_prob = lib.calc_probs_single(tweets, category, eng_stopwords)\n",
    "    probs[category] = (prior_prob, token_prob)\n",
    "\n",
    "# Get average F1 score for the test set\n",
    "predictions = [(tweet, lib.classify_nb_single(tweet, probs, eng_stopwords)) for tweet in test_tweets] # maps each test tweet to its predicted label\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these results with the previous ones. Does stop word removal help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Remember that the goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "A difference between stemming and lemmatization is that stemming looks at the current word only, while lemmatization also takes the context into consideration. Either way, this pre-processing step could be somewhat tedious. Luckily, the powerful `nltk` provides tools for both.\n",
    "\n",
    "### Stemming using the Porter stemmer\n",
    "*Porter's algorithm*, developed in the 1980s, is a classic stemmer even used today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "# Get the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Let's stemming on plurals\n",
    "plurals = ['apples', 'batteries', 'generators', 'medicines', 'tests', 'feet']\n",
    "print('plurals:')\n",
    "for plural in plurals:\n",
    "    print('{:s} --> {:s}'.format(plural, stemmer.stem(plural)))\n",
    "print()\n",
    "    \n",
    "# and variations of verbs\n",
    "verbs = ['studies', 'thinks', 'goes', 'played', 'bought', 'went', 'ran', 'drew', ]\n",
    "print('verbs:')\n",
    "for verb in verbs:\n",
    "    print('{:s} --> {:s}'.format(verb, stemmer.stem(verb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more words to `plurals` and see what the stemming results look like.  \n",
    "You may find that the results may look a bit mechanical. This is because the Porter's algorithm is essentially a sequential application of a set of rules. To get better looking results, let's try out a lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following line when you this cell for the first time:\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Get the lemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the plurals\n",
    "print('plurals:')\n",
    "for plural in plurals:\n",
    "    print('{:s} --> {:s}'.format(plural, lmtzr.lemmatize(plural)))\n",
    "print()\n",
    "\n",
    "# Lemmatize the verbs\n",
    "print('verbs:')\n",
    "for verb in verbs:\n",
    "    print('{:s} --> {:s}'.format(verb, lmtzr.lemmatize(verb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet perfect, but much better, especially for the plurals. Whoray! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's check whether stemming or lemmatization can help with our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "probs = {}\n",
    "for category in categories:\n",
    "    prior_prob, token_prob = lib.calc_probs_single(tweets, category, stemmer=stemmer)\n",
    "    probs[category] = (prior_prob, token_prob)\n",
    "\n",
    "# Get average F1 score for the test set\n",
    "predictions = [(tweet, lib.classify_nb_single(tweet, probs, stemmer=stemmer)) for tweet in test_tweets] # maps each test tweet to its predicted label\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "probs = {}\n",
    "for category in categories:\n",
    "    prior_prob, token_prob = lib.calc_probs_single(tweets, category, lmtzr=lmtzr)\n",
    "    probs[category] = (prior_prob, token_prob)\n",
    "\n",
    "# Get average F1 score for the test set\n",
    "predictions = [(tweet, lib.classify_nb_single(tweet, probs, lmtzr=lmtzr)) for tweet in test_tweets] # maps each test tweet to its predicted label\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some improvement, not bad! \n",
    "\n",
    "Now let's try using these tricks together, i.e. combining stop words removal with stemming or lemmatization. We don't need both stemming and lemmatization since they are two alternatives serving the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word removal + stemming\n",
    "probs = {}\n",
    "for category in categories:\n",
    "    prior_prob, token_prob = lib.calc_probs_single(tweets, category, stop_words=eng_stopwords, stemmer=stemmer)\n",
    "    probs[category] = (prior_prob, token_prob)\n",
    "\n",
    "# Get average F1 score for the test set\n",
    "predictions = [(tweet, lib.classify_nb_single(tweet, probs, stop_words=eng_stopwords, stemmer=stemmer)) for tweet in test_tweets] # maps each test tweet to its predicted label\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word removal + Lemmatization\n",
    "probs = {}\n",
    "for category in categories:\n",
    "    prior_prob, token_prob = lib.calc_probs_single(tweets, category, stop_words=eng_stopwords, lmtzr=lmtzr)\n",
    "    probs[category] = (prior_prob, token_prob)\n",
    "\n",
    "# Get average F1 score for the test set\n",
    "predictions = [(tweet, lib.classify_nb_single(tweet, probs, stop_words=eng_stopwords, lmtzr=lmtzr)) for tweet in test_tweets] # maps each test tweet to its predicted label\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does using several tricks together always work better using one of them alone? Why do you think is the case?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
