{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLTp_tYi8O2t"
   },
   "source": [
    "# Regression\n",
    "\n",
    "Written by Judy Hanwen Shen 2021  \n",
    "\n",
    "Adapted from Terron Ishihara "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_QA59cN8UT8"
   },
   "source": [
    "### Problem 1\n",
    "\n",
    "You decide to use linear regression to predict the battery life of the phone. The features you pick are $x_1$ = phone weight and $x_2$ = screen size.  The linear regression model with two features and two weights is given by:\n",
    "\n",
    "> $\\hat{y} = w_1x_1 + w_2x_2$\n",
    "\n",
    "| Weight (ounces) | Screen Size (squared inches) | Battery Life (hours) |\n",
    "|-|-|-|\n",
    "| 6 | 4 | 12 |\n",
    "| 13 | 16 | 24 |\n",
    "| 5 | 15 | 20 |\n",
    "| 9 | 48 | 21 |\n",
    "| 7 | 13 | 16 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GlxhnZ42e2R"
   },
   "source": [
    "> a. You initially guess $w_1=3$ and $w_2=4$. What is the predicted battery life for a phone of weight 10 ounces and screen size 18 squared inches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtmJUbH-RSlo"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "w1 = \n",
    "w2 = \n",
    "weight = \n",
    "screen_size = \n",
    "\n",
    "y_guess = \n",
    "print(y_guess)\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuhTpatE3NwI"
   },
   "source": [
    "> b. Recall the formula for Mean Squared Error (MSE) for this model is:\n",
    "\n",
    "> > $\\frac{1}{N}\\sum_{i=1}^N (w_1x_1+w_2x_2-y)^2$\n",
    "\n",
    "> What is the MSE for the model with $w_1=3$ and $w_2=4$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4K5ktadp97e"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "# Step 1: write mse helper function\n",
    "def mse(y_guess, y_true): \n",
    "    pass \n",
    "\n",
    "# Step 2: write x1, x2 and y_true as arrays \n",
    "\n",
    "\n",
    "# calculate y_pred \n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(\"The mean squared error is: \", mse(y_pred, y_true))\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtGpYBam3srZ"
   },
   "source": [
    "> c. The update functions for the weights are:\n",
    "\n",
    ">> $w_1 = w_1 - \\alpha * (w_1x_1+w_2x_2-y)* x_1$\n",
    "\n",
    ">> $w_2 = w_2 - \\alpha * (w_1x_1+w_2x_2-y)* x_2$\n",
    "\n",
    "> Using the first data point as your example $([x_1,x_2], y)$ and setting $\\alpha=0.001$, what are the new weights after one round of updates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUfiL-sdRJ_U"
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "print(\"New w1: \", w1)\n",
    "print(\"New w2: \", w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqGPT8Z-4RDc"
   },
   "source": [
    "> d. What is the new MSE after one round of updates? Did the MSE increase or decrease after updating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDKxXrF_RVYY"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "# calculate y_pred \n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "print(\"y pred:\", y_pred)\n",
    "print(\"y true:\", y_true)\n",
    "\n",
    "print(\"The mean squared error is: \", mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e. Iterate through all the points and see how low you can get the mse (Hint: Try to decrement the learning rate alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "num_iter = 10\n",
    "w1 = 3\n",
    "w2 = 4\n",
    "mse_arr = []\n",
    "for k in range(num_iter): \n",
    "    print(\"Iteration \", k)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "    #### END CODE HERE ####\n",
    "    print(\"New w1: \", w1, \"New w2: \", w2)\n",
    "    print(\"y pred:\", np.round(y_pred, 1))    \n",
    "    print(\"y true:\", y_true)\n",
    "    error = mse(y_pred, y_true)\n",
    "    mse_arr.append(error)\n",
    "    print(\"The mean squared error is: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_iter), mse_arr, marker='x')\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Number of iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scl1uXhWKNnu"
   },
   "source": [
    "### Problem 2\n",
    "\n",
    "Let's explore regression using scikit-learn. We'll use a [dataset on diabetes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html), with 442 samples and 10 different features. We will focus on using a single feature, body mass index. The target values are a quantitative measure of diabetes progression one year after baseline.\n",
    "\n",
    "> To start, we import the models, the dataset, and some extra tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_g_iWxKLWgK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "# Get the data samples and extract the feature at index 2,\n",
    "# which is body mass index\n",
    "X = diabetes.data[:, np.newaxis, 2]\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLTHmM3qu0tz"
   },
   "source": [
    "> Now we can split our dataset into training and test sets. The `train_test_split()` method can do this for us. Here we use an 80/20 split. Then we create the regression classifiers, train them on the training set, and evaluate the models on the test set, printing out the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQuzrYj_urRP"
   },
   "outputs": [],
   "source": [
    "# Partition the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "# Instantiate the classifiers\n",
    "lin_clf = LinearRegression()\n",
    "# The parameters here are the default values for LogisticRegression,\n",
    "# included here just to suppress a couple warnings\n",
    "log_clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Train the classifiers\n",
    "lin_clf.fit(X_train, y_train)\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate predictions on test set\n",
    "lin_predictions = lin_clf.predict(X_test)\n",
    "log_predictions = log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL_K9_t1vMOc"
   },
   "source": [
    "> To get a better idea of what these models look like, let's plot the models' predictions on top of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKnm1BIyYljY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "# Plot the dataset points\n",
    "\n",
    "\n",
    "# Plot the linear regression predictions\n",
    "\n",
    "\n",
    "# Plot the logistic regression predictions\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "\n",
    "# Label axes and legend\n",
    "plt.xlabel(\"Body Mass Index\")\n",
    "plt.ylabel(\"Disease progression after one year\")\n",
    "plt.legend(('Linear', 'Logistic'),\n",
    "           loc=\"lower right\", fontsize='small')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySL0pqapwAfp"
   },
   "source": [
    "> Recall the formula for the error function of our linear regression model, as an example: $w_1x+w_0-y$. We take the input, $x$, place it into our linear model, $w_1x+w_0$, and subtract the true value, $y$. A useful measure of how closely our model predicts all the test samples is Mean Squared Error (MSE). We simply square all of our error values, sum those squares, then divide by $N$ to calculate the mean squared error:\n",
    "\n",
    ">> $\\frac{1}{N}\\sum_{i=1}^N (w_1x+w_0-y)^2$\n",
    "\n",
    "> The logistic regression model has the same calculation for MSE, but with the sigmoid function as the model: $\\frac{1}{1+e^{-(w_1x+w_0)}}$.\n",
    "\n",
    "> Given the above plot, which regression classifier do you think has a larger mean squared error? Check your answer by printing out the values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcQhWFylvnXW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "# Calculate the linear regression MSE and save it in a variable called lin_mse\n",
    "\n",
    "\n",
    "# Calculate the logistic regression MSE and save it in a variable called lin_mse\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "print(\"Sklearn linear mse: \", abs(mean_squared_error(lin_predictions, y_test)))\n",
    "print(\"Your linear mse: \", lin_mse)\n",
    "      \n",
    "print(\"Sklearn logistic mse: \", abs(mean_squared_error(log_predictions, y_test)))\n",
    "print(\"Your logistic mse: \", log_mse)\n",
    "\n",
    "assert (abs(mean_squared_error(lin_predictions, y_test) - lin_mse) < 1e-6)\n",
    "assert (abs(mean_squared_error(log_predictions, y_test) - log_mse) < 1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13dtpqh1ZWmV"
   },
   "source": [
    "## Problem 3:  Predicting housing prices with regression \n",
    "## From scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsW9S_ulYUXm"
   },
   "source": [
    "Source: [Article](https://towardsdatascience.com/predicting-house-prices-with-linear-regression-machine-learning-from-scratch-part-ii-47a0238aeac1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWQVJtT2uBSG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "import unittest\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def run_tests():\n",
    "  unittest.main(argv=[''], verbosity=1, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W7kXJn1Sh8a"
   },
   "source": [
    "# Load the data\n",
    "\n",
    "Data [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RA46huZgyOj"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Data-Science-FMI/ml-from-scratch-2019/master/data/house_prices_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_jaLzJnqT_y"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('house_prices_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X60KNF9RTyEu"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R1HDRhRBcKl"
   },
   "source": [
    "Our goal is going to be to predict the SalePrice column. Let's take a look at what that data looks like first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YSv7SqUR6bd"
   },
   "outputs": [],
   "source": [
    "df_train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7ykRIwOVxBa"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df_train['SalePrice']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo3hYDpnBiWR"
   },
   "source": [
    "Next, let's see how the size of the living area compares to the sale price. How predictive is the living area metric? Are there outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A7RjShcV7qm"
   },
   "outputs": [],
   "source": [
    "var = 'GrLivArea'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), s=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-hl6z2rBlVi"
   },
   "source": [
    "Next, we will explore the basement size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XgD7MrdWAoJ"
   },
   "outputs": [],
   "source": [
    "var = 'TotalBsmtSF'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxW_eG84Boec"
   },
   "source": [
    "Lastly, we will take a look at the OverallQual metric which is a more subjective feature and describes the overall material quality and finish of the house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOqgVkVrWIHB"
   },
   "outputs": [],
   "source": [
    "var = 'OverallQual'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(14, 8))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__rKe8cFBtRk"
   },
   "source": [
    "Let's see which features are most correlated with the SalePrice feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9oqWbmaWWjn"
   },
   "outputs": [],
   "source": [
    "corrmat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGetIp9sWtdo"
   },
   "outputs": [],
   "source": [
    "k = 9 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "f, ax = plt.subplots(figsize=(14, 10))\n",
    "sns.heatmap(df_train[cols].corr(), vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7RoixG1Y0YC"
   },
   "outputs": [],
   "source": [
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars']\n",
    "sns.pairplot(df_train[cols], size = 4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6nLbr0bZN6y"
   },
   "source": [
    "## Do we have missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPfBNvltZ7Cl"
   },
   "outputs": [],
   "source": [
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1l9DsgTcJtI"
   },
   "source": [
    "# Predicting the sale price\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "### Feature scaling\n",
    "\n",
    "We will do a little preprocessing to our data using the following formula (standardization):\n",
    "\n",
    "$$x'= \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu$ is the population mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*CY7FB1ccGJOhAPdhICpKig.jpeg)\n",
    "\n",
    "**Source: Andrew Ng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmEcYWkaqYDg"
   },
   "outputs": [],
   "source": [
    "x = df_train['GrLivArea']\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "x = (x - x.mean()) / x.std()\n",
    "x = np.c_[np.ones(x.shape[0]), x] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSVrJbO2cNr1"
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "![](https://i.ytimg.com/vi/zPG4NjIkCjc/maxresdefault.jpg)\n",
    "\n",
    "**Source: MyBookSucks**\n",
    "\n",
    "Linear regression models assume that the relationship between a dependent continuous variable $Y$ and one or more explanatory (independent) variables $X$ is linear (that is, a straight line). It’s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). Linear regression models can be divided into two main types:\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression uses a traditional slope-intercept form, where $a$ and $b$ are the coefficients that we try to “learn” and produce the most accurate predictions. $X$ represents our input data and $Y$ is our prediction.\n",
    "\n",
    "$$Y = bX + a$$\n",
    "\n",
    "![](https://spss-tutorials.com/img/simple-linear-regression-equation-linear-relation.png)\n",
    "\n",
    "**Source: SPSS tutorials**\n",
    "\n",
    "### Multivariable Regression\n",
    "\n",
    "A more complex, multi-variable linear equation might look like this, where w represents the coefficients, or weights, our model will try to learn.\n",
    "\n",
    "$$ Y(x_1,x_2,x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_0$$\n",
    "\n",
    "The variables $x_1, x_2, x_3$ represent the attributes, or distinct pieces of information, we have about each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9SjF0vba6kw"
   },
   "source": [
    "## Loss function\n",
    "\n",
    "Given our Simple Linear Regression equation:\n",
    "\n",
    "$$Y = bX + a$$\n",
    "\n",
    "We can use the following cost function to find the coefficients:\n",
    "\n",
    "### Mean Squared Error (MSE) Cost Function\n",
    "\n",
    "The MSE is defined as:\n",
    "\n",
    "$$MSE = J(W) =  \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - h_w(x^{(i)}))^2$$\n",
    "\n",
    "where\n",
    "\n",
    "$$h_w(x) = g(w^Tx)$$\n",
    "\n",
    "The MSE measures how much the average model predictions vary from the correct values. The number is higher when the model is performing \"bad\" on the training set.\n",
    "\n",
    "The first derivative of MSE is given by:\n",
    "\n",
    "$$MSE' = J'(W) = \\frac{2}{m} \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})$$\n",
    "\n",
    "\n",
    "### One Half Mean Squared Error (OHMSE)\n",
    "\n",
    "We will apply a small modification to the MSE - multiply by $\\frac{1}{2}$ so when we take the derivative, the `2`s cancel out:\n",
    "\n",
    "$$ OHMSE = J(W) =  \\frac{1}{2m} \\sum_{i=1}^{m} (y^{(i)} - h_w(x^{(i)}))^2 $$\n",
    "\n",
    "The first derivative of OHMSE is given by:\n",
    "\n",
    "$$OHMSE' = J'(W) = \\frac{1}{m} \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PFCAqS2atC3"
   },
   "outputs": [],
   "source": [
    "# Return the MSE loss \n",
    "def loss(h, y):\n",
    "  #### YOUR CODE HERE ####\n",
    "\n",
    "  #### YOUR CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6MWAKGYlSQY"
   },
   "outputs": [],
   "source": [
    "class TestLoss(unittest.TestCase):\n",
    "\n",
    "  def test_zero_h_zero_y(self):\n",
    "    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([0])), 0)\n",
    "\n",
    "  def test_one_h_zero_y(self):\n",
    "    self.assertAlmostEqual(loss(h=np.array([1]), y=np.array([0])), 0.5)\n",
    "\n",
    "  def test_two_h_zero_y(self):\n",
    "    self.assertAlmostEqual(loss(h=np.array([2]), y=np.array([0])), 2)\n",
    "    \n",
    "  def test_zero_h_one_y(self):\n",
    "    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([1])), 0.5)\n",
    "    \n",
    "  def test_zero_h_two_y(self):\n",
    "    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([2])), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaOPbVF2lT4j"
   },
   "outputs": [],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0S9_IiDCCwa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG2ss3oyFOuX"
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "  #### YOUR CODE HERE ####\n",
    "\n",
    "  # Return predicted label given input X\n",
    "  # Hint: y_pred = W^T x \n",
    "  def predict(self, X):\n",
    "    #### YOUR CODE HERE ####\n",
    "    #### END CODE HERE ####\n",
    "    return predicted_value \n",
    "  \n",
    "  def _gradient_descent_step(self, X, targets, lr):\n",
    "\n",
    "    # Get your predicted value for input X \n",
    "    predictions = self.predict(X)\n",
    "    \n",
    "    # Calculate the gradient \n",
    "    error = predictions - targets\n",
    "    gradient = np.dot(X.T,  error) / len(X)\n",
    "\n",
    "    # Update the weight vector self._W \n",
    "    self._W -= lr * gradient\n",
    "\n",
    "      \n",
    "  def fit(self, X, y, n_iter=100000, lr=0.01):\n",
    "\n",
    "    self._W = np.zeros(X.shape[1])\n",
    "\n",
    "    self._cost_history = []\n",
    "    self._w_history = [self._W]\n",
    "    for i in range(n_iter):\n",
    "      \n",
    "        # Get your predicted value y_pred \n",
    "        prediction = self.predict(X)\n",
    "\n",
    "        # Get your loss value given the true label y and your prediction \n",
    "        cost = loss(prediction, y)\n",
    "        \n",
    "        # Add your loss value to the self._cost_history list so we can \n",
    "        # plot it later \n",
    "        self._cost_history.append(cost)\n",
    "        \n",
    "        # Perform gradient descent \n",
    "        self._gradient_descent_step(X, y, lr)\n",
    "        \n",
    "        self._w_history.append(self._W.copy())\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGWFw7yCrNj6"
   },
   "outputs": [],
   "source": [
    "class TestLinearRegression(unittest.TestCase):\n",
    "\n",
    "    def test_find_coefficients(self):\n",
    "      clf = LinearRegression()\n",
    "      clf.fit(x, y, n_iter=2000, lr=0.01)\n",
    "      np.testing.assert_array_almost_equal(clf._W, np.array([180921.19555322,  56294.90199925]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgbjeZbHrtz8"
   },
   "outputs": [],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dl5Io9hMJRAm"
   },
   "outputs": [],
   "source": [
    "clf = LinearRegression()\n",
    "clf.fit(x, y, n_iter=2000, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cmILFdpJ0MS"
   },
   "outputs": [],
   "source": [
    "clf._W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjyPaS_iM9n1"
   },
   "outputs": [],
   "source": [
    "clf._cost_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2FeoJ6aOZlX"
   },
   "outputs": [],
   "source": [
    "plt.title('Cost Function J')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(clf._cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D382iNZEO0Zo"
   },
   "outputs": [],
   "source": [
    "#Animation\n",
    "\n",
    "#Set the plot up,\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.title('Sale Price vs Living Area')\n",
    "plt.xlabel('Living Area in square feet (normalised)')\n",
    "plt.ylabel('Sale Price ($)')\n",
    "plt.scatter(x[:,1], y)\n",
    "line, = ax.plot([], [], lw=2, color='red')\n",
    "annotation = ax.text(-1, 700000, '')\n",
    "annotation.set_animated(True)\n",
    "plt.close()\n",
    "\n",
    "#Generate the animation data,\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    annotation.set_text('')\n",
    "    return line, annotation\n",
    "\n",
    "# animation function.  This is called sequentially\n",
    "def animate(i):\n",
    "    x = np.linspace(-5, 20, 1000)\n",
    "    y = clf._w_history[i][1]*x + clf._w_history[i][0]\n",
    "    line.set_data(x, y)\n",
    "    annotation.set_text('Cost = %.2f e10' % (clf._cost_history[i]/10000000000))\n",
    "    return line, annotation\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=300, interval=10, blit=True)\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRxiVvutuQ30"
   },
   "source": [
    "# Multivariable Linear Regression\n",
    "\n",
    "Let's use more of the available data to build a Multivariable Linear Regression model and see whether or not that will improve our OHMSE error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7COJ54iJuQTC"
   },
   "outputs": [],
   "source": [
    "x = df_train[['OverallQual', 'GrLivArea', 'GarageCars']]\n",
    "\n",
    "x = (x - x.mean()) / x.std()\n",
    "x = np.c_[np.ones(x.shape[0]), x] \n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(x, y, n_iter=2000, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cW68k8Duu1Tw"
   },
   "outputs": [],
   "source": [
    "clf._W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZcgpItsu5GO"
   },
   "outputs": [],
   "source": [
    "plt.title('Cost Function J')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(clf._cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jv4q_lPQu80b"
   },
   "outputs": [],
   "source": [
    "clf._cost_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Disaster Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_labeled-data-singlelabels-train.csv'\n",
    "test_url  = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_labeled-data-singlelabels-test.csv'\n",
    "df_train = pd.read_csv(train_url)\n",
    "df_test  = pd.read_csv(test_url)\n",
    "\n",
    "train_tweets  = df_train['Text']\n",
    "train_classes = df_train['Class']\n",
    "\n",
    "test_tweets  = df_test['Text']\n",
    "test_classes = df_test['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vec_train_tweets = vectorizer.fit_transform(train_tweets)\n",
    "vec_test_tweets  = vectorizer.transform(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_classifier = LogisticRegression(random_state=0).fit(vec_train_tweets, train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of your classifier by using the LogisticRegression library's score function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Hint: use the score function lg_classifier.score()\n",
    "\n",
    "#### END CODE HERE ####\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average f1 score of your classifier by using f1_score() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Hint: use the fl_score function \n",
    "\n",
    "lg_predicted_classes = lg_classifier.predict(vec_test_tweets)\n",
    "score = f1_score(test_classes, lg_predicted_classes, labels=['Food','Water','Energy','Medical','None'], average=None)\n",
    "average_f1_score = np.average(score)\n",
    "\n",
    "#### END CODE HERE ####\n",
    "print(score)\n",
    "print(average_f1_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear Regression",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
