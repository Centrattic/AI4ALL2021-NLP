{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-V_jS65FyA-5"
   },
   "outputs": [],
   "source": [
    "# Run this every time you open the spreadsheet\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5Ea6C4cInZc"
   },
   "source": [
    "### Problem 1\n",
    "\n",
    "Let's first observe how scikit-learn's KMeans classifier works on some randomly generated data. We'll make use of the `make_blobs()` method in `sklearn.datasets`, which randomly generates points that naturally fit into groups (called 'blobs'). This gives us a good dataset to demonstrate the results of KMeans clustering.\n",
    "\n",
    "> We start by importing all the libraries and methods we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VdAE6tCJRvS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4S0nhojNOzi"
   },
   "source": [
    "> Now we can use `make_blobs()` to, well, make some blobs. We can specify how many points to generate with `n_samples` and how many blobs to make with `centers`. `random_state` is a seed for the random number generator. Here, we specify that 5 blobs should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_v1kG2UaNRLp"
   },
   "outputs": [],
   "source": [
    "# random_state can be any integer value. It is used as a seed for\n",
    "# the random generator so that we get the same random results every\n",
    "# time. Without this specified, each run of this code would produce\n",
    "# a new random set of values. Feel free to change this value to see\n",
    "# what different results show up.\n",
    "random_state = 42\n",
    "# n_samples specifies how many points to generate.\n",
    "# centers specifies how many blobs to make.\n",
    "X, y = make_blobs(n_samples=2000, centers=5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYSIAN6LNxsG"
   },
   "source": [
    "> Let's see what this looks like by plotting the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJHp_dCtNzZd"
   },
   "outputs": [],
   "source": [
    "# Make the plot a relatively small size\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the blobs\n",
    "# The first and second column of X are the x- and y- values respectively.\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Random blobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9Wqm4aUN_bE"
   },
   "source": [
    "> There are clearly 5 clusters here. Just to make things interesting, let's first classify this data with a `k` value of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZONYqIFNVVP"
   },
   "outputs": [],
   "source": [
    "# Create our KMeans classifier\n",
    "# n_clusters specifies what value of k to use (how many clusters)\n",
    "kmeans_clf = KMeans(n_clusters=3, random_state=random_state)\n",
    "\n",
    "# fit_predict does two things: fits the classifier using X\n",
    "# and then predicts the clusters, assigning an index to each cluster.\n",
    "# You can also call fit() and predict() separately.\n",
    "y_pred = kmeans_clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTZkFBlOQMk"
   },
   "source": [
    "> Plot the data points, but with different colors for each of the clusters we determined using KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh3CmIWiNXBt"
   },
   "outputs": [],
   "source": [
    "# Make the plot a relatively small size\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the results\n",
    "# c specifies the colors of each point if given an array (as we do here).\n",
    "# A new color will be used for each value in the given array, so each\n",
    "# point in X is given a color corresponding to the predicted value in y_pred.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('KMeans Example - Random Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ80x-PHOYOs"
   },
   "source": [
    "> The results are pretty good given that we gave our KMeans classifier an incorrect value for `k`. Try changing the value of `n_clusters` for our KMeans classifier above to 5 and see what the results are. Do the clusters match what you would expect? How about 10 clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3vkct82Oylx"
   },
   "source": [
    "### Problem 2\n",
    "\n",
    "Let's see how KMeans can be used on real data now by using our twitter dataset. \n",
    "\n",
    "First, we'll import the required libaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yk3y4XDNv2S1"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoOlEdF19HQ-"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_labeled-data-singlelabels-train.csv'\n",
    "df_data = pd.read_csv(url)\n",
    "\n",
    "tweets_text  = df_data['Text']\n",
    "tweets_class = df_data['Class']\n",
    "tweets_type  = df_data['Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-bQyoH3O3VB"
   },
   "source": [
    "Next, we'll load our data using pandas.\n",
    "\n",
    "Our next step is to convert our twitter data from a list of strings to a matrix with numbers. We'll go over why and how to represent text from our tweets with numerical data in vectorized form later in the course, but for now, just assume that we use some mapping (in this case this mapping is implemented by the TfidfVectorizer class) to transform the text in our tweets to a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHBMDOCqacFq"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_labeled-data-singlelabels-train.csv'\n",
    "df_data = pd.read_csv(url)\n",
    "\n",
    "tweets_text  = df_data['Text']\n",
    "tweets_class = df_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO8RzVmNbBfn"
   },
   "outputs": [],
   "source": [
    "# Convert the list of text from the tweets into a vectorized form\n",
    "vectorizer = TfidfVectorizer()\n",
    "vec_train_tweets = vectorizer.fit_transform(tweets_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HM6cAQqR1bq"
   },
   "source": [
    "## Select the number of clusters\n",
    "\n",
    "Next, we need to figure out how many clusters we need for our data. In problem 1, it was easy to see that we had 5 clusters in the data, but in this case, we don't know how many clusters are in the data. \n",
    "\n",
    "To tune this parameter, we will test the performance of our model with different number of clusters and select the best one. \n",
    "\n",
    "We're going to use two metrics to evaluate the performance of the model. \n",
    "* **Homogeneity score**: This metric describes what percentage of each cluster belongs to the same class. This metric assumes you have access to the true labels. \n",
    "* **Silhouette score**: This metric returns a measure of how similar a data point is to the cluster it is a part of compared to the other clusters. If our model is performing well, then data points in a single cluster should be very similar, so we want a high silhouette score. \n",
    "* **Inertia**: This metric calculates the distance between data points within a given cluster. Because we want our data points in a given cluster to be close to each other, we want the inertia to be small. \n",
    "\n",
    "We've created a list of cluster sizes called kNumClusters, and we are going to iterate through each size in the list and test how well the model performs with this cluster size. \n",
    "\n",
    "**Fun tip**: \n",
    "The for loop below that iterates through each element of the kNumClusters wraps kNumClusters with tqdm.tqdm. This is a library that will display a progress bar when you execute the code block displaying how many iterations you've finished in the for loop. Given that the kmeans clustering algorithm can take a while for larger number of clusters, it's nice to know how much progress we've made. \n",
    "\n",
    "Your task: Using the sample code in problem 1, create a kmeans model and fit it to the training data (vec_train_tweets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the metrics yourself: \n",
    "\n",
    "# Homogeneity score\n",
    "print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0]))\n",
    "print(\"%.6f\" % homogeneity_score([1, 1, 0, 0], [1, 1, 0, 0]))\n",
    "print(\"%.6f\" % homogeneity_score([1, 0, 1, 0], [1, 1, 0, 0]))\n",
    "\n",
    "silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YEDkJo31TLv"
   },
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "homogeneity_scores = []\n",
    "inertias = []\n",
    "\n",
    "kNumClusters = [3, 5, 50, 200, 500, 1000]  # list of cluster sizes to try\n",
    "\n",
    "for k in tqdm.tqdm(kNumClusters):\n",
    "    ### YOUR CODE HERE ####\n",
    "    # Add a line to create the model. Pay attention to what the n_clusters \n",
    "    # parameter should be \n",
    "\n",
    "\n",
    "    # Add a line to run the kmeans algorithm on our data, vec_train_tweets\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ####\n",
    "    silhouette_scores.append(silhouette_score(vec_train_tweets, model.labels_))\n",
    "    inertias.append(model.inertia_)\n",
    "    homogeneity_scores.append(homogeneity_score(tweets_class, model.predict(vec_train_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdlRb9k5Y38Y"
   },
   "source": [
    "Next, we'll plot our silhouette score and inertia metric to see which number of clusters is best for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZzEPAJG1ybw"
   },
   "outputs": [],
   "source": [
    "# plot the quality metrics for inspection\n",
    "fig, ax = plt.subplots(3, 1, sharex=True)\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(kNumClusters, inertias, 'o--')\n",
    "plt.ylabel('inertia')\n",
    "plt.title('Kmeans Parameter Search')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(kNumClusters, silhouette_scores, 'o--')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.xlabel('Number of clusters');\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(kNumClusters, homogeneity_scores, 'o--')\n",
    "plt.ylabel('homogeneity score')\n",
    "plt.xlabel('Number of clusters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of tweets\", len(tweets_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhok1lqRHlJa"
   },
   "source": [
    "Our model doesn't seem to perform very well. Let's take a look at our data with a technique called Principle Component Analysis (PCA) and see what's going on. PCA allows us to reduce the features of the vectorized embeddings of our tweet into 2D to allow us to visualize it. The plot assigns a color to each tweet according to the true class it belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fIjh_YPHktl"
   },
   "outputs": [],
   "source": [
    "# reduce the features to 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(vec_train_tweets.toarray())\n",
    "\n",
    "class_dict = {\"None\":0, \"Food\":1, \"Water\":2, \"Medical\": 3, \"Energy\": 4}\n",
    "class_list = [class_dict[x] for x in tweets_class]\n",
    "\n",
    "plt.scatter(reduced_features[:,0], reduced_features[:,1], c=class_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu4-RxwZIOkx"
   },
   "source": [
    "Can you identify clear clusters in our data given this embedding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq6ouQ2xZDMH"
   },
   "source": [
    "**Pick a number for the number of clusters** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hr3xcQExZBPE"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTbidGu8aEAD"
   },
   "source": [
    "Next, we'll train a Kmeans model on our data using the number of clusters you've selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsXJG5Z92U1L"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnbXqTaOaU-p"
   },
   "source": [
    "## Investigate the clusters in our data\n",
    "\n",
    "First, we'll look at the how many data points are in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPeecxPk4KOP"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(len(set(km_model.labels_))), np.bincount(km_model.labels_))\n",
    "\n",
    "plt.ylabel('Number of data points')\n",
    "plt.xlabel('Cluster label')\n",
    "plt.title('Number of data points in each cluster');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsRa1Ge2wXOQ"
   },
   "source": [
    "Next, let's see what the distribution of tweet categories are in each cluster. How well does our model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQRJTdsDpTSx"
   },
   "outputs": [],
   "source": [
    "tweet_labels_dict = defaultdict(list)\n",
    "\n",
    "for i,label in enumerate(km_model.labels_): \n",
    "    tweet_labels_dict[label].append(tweets_class[i])\n",
    "\n",
    "list_classes = ['None', 'Food', 'Energy', 'Medical', 'Water']\n",
    "cluster_df = pd.DataFrame(columns=list_classes)\n",
    "\n",
    "list_class_dicts = []\n",
    "for i,key in enumerate(tweet_labels_dict.keys()): \n",
    "    n = len(tweet_labels_dict[key])\n",
    "    class_dict = defaultdict(int)\n",
    "    for val in tweet_labels_dict[key]: \n",
    "        class_dict[val] += 1\n",
    "    list_class_dicts.append(class_dict)\n",
    "\n",
    "    output = \"Cluster # \" + str(i) + \" is \"\n",
    "    cluster_dict = {} \n",
    "    max_percentage = 0\n",
    "    for resource_class in list_classes: \n",
    "        percentage = round(class_dict[resource_class]/n,2)*100\n",
    "        max_percentage = max(max_percentage, percentage)\n",
    "        cluster_dict[resource_class] = class_dict[resource_class]\n",
    "        output += str(percentage) + \"% class \" + resource_class + \", \"\n",
    "    print(output)\n",
    "    cluster_df.loc[len(cluster_df.index)] = cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cluster_df.plot.bar(stacked=True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1c3pVFoJxVU"
   },
   "source": [
    "Next, let's visualize our clusters using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3NEEwY3OBOR"
   },
   "outputs": [],
   "source": [
    "# reduce the features to 2D\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "reduced_features = pca.fit_transform(vec_train_tweets.toarray())\n",
    "\n",
    "# reduce the cluster centers to 2D\n",
    "reduced_cluster_centers = pca.transform(km_model.cluster_centers_)\n",
    "\n",
    "# plot the clusters\n",
    "plt.scatter(reduced_features[:,0], reduced_features[:,1], c=km_model.predict(vec_train_tweets))\n",
    "plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IBbHVSjKJDB"
   },
   "source": [
    "How does this compare to the previous plot with the previous labels? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S8lWrRkdKk0"
   },
   "source": [
    "Let's take a look at the text inside each cluster. We'll use the get_cluster_sample function to print out text from a given cluster. Change the cluster number and observe what text output you get. What patterns do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlzebuNY8pm3"
   },
   "outputs": [],
   "source": [
    "def get_cluster_sample(orig_text, model, tweet_index, preview=15):\n",
    "    \"\"\"\n",
    "    Display original text for tweets in cluster with tweet_index\n",
    "    \"\"\"\n",
    "    for i,idx in enumerate(np.where(model.labels_ == tweet_index)[0]):\n",
    "        print(orig_text[idx].replace('\\n',' '))\n",
    "        print()\n",
    "        if i > preview:\n",
    "            print('( >>> truncated preview <<< )')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4NS-zQQ-HlF"
   },
   "outputs": [],
   "source": [
    "num_cluster = 2\n",
    "get_cluster_sample(tweets_text, km_model, num_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qg_gugQ_VtWL"
   },
   "source": [
    "References: \n",
    "https://twitterdev.github.io/do_more_with_twitter_data/clustering-users.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJShSfzhWKuO"
   },
   "source": [
    "### Challenge problem: Implement kmeans on the blob dataset (from problem 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suKB7f5-MEWi"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "X, y = make_blobs(n_samples=2000, centers=5, random_state=random_state)\n",
    "\n",
    "'''\n",
    "Function to calculate the Euclidean distance between two points \n",
    "'''\n",
    "def get_distance_btw_pts(pt1, pt2): \n",
    "    distance = np.linalg.norm(pt1-pt2)\n",
    "    return distance \n",
    "\n",
    "'''\n",
    "Return the index of the cluser (if you have five clusters this will be a \n",
    "number between 0 and 4) to whom the data point is closest \n",
    "Inputs \n",
    "- X: data \n",
    "- pt_index: index of the point you want the closest center to \n",
    "- centers: list of the coordinates of the centers \n",
    "'''\n",
    "def get_closest_center(X, pt_coord, centers): \n",
    "  ### YOUR CODE HERE ####\n",
    "\n",
    "  ### YOUR CODE HERE ####\n",
    "    return closest_center_index\n",
    "\n",
    "'''\n",
    "Return the centroid of a cluster\n",
    "Inputs \n",
    "- X: data \n",
    "- labels: list of cluster indices that correspond to the data \n",
    "- cluster_num: index of the cluster we want the center of \n",
    "'''\n",
    "def get_cluster_center(X, labels, cluster_num): \n",
    "  ### YOUR CODE HERE ####\n",
    "\n",
    "  ### END CODE HERE ####\n",
    "    return cluster_center\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # Step 1: Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    centers_coords = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[centers_coords]\n",
    "    \n",
    "    while True:  \n",
    "        # Step 2: Assign labels based on closest center\n",
    "        labels = []\n",
    "        for j in range(X.shape[0]): \n",
    "            label = get_closest_center(X, j, centers)\n",
    "            labels.append(label)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Step 3: Find new centers from means of points\n",
    "        new_centers = []\n",
    "        for i in range(n_clusters): \n",
    "            new_centers.append(get_cluster_center(X, labels, i))\n",
    "        new_centers = np.array(new_centers)\n",
    "\n",
    "        # Step 4: Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64cM55JZVflJ"
   },
   "source": [
    "Can you come up with more pythonic syntax for steps 2 and 3?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Clustering",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
