{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Data Representation \n",
    "\n",
    "This activity will be divided into three parts. \n",
    "1. We're going to get some practice with n-gram language models.  \n",
    "2. We'll introduce ways we can do error analysis on the Naive Bayes classifier. \n",
    "3. We're going to do a mini-project where we'll investigate ways to try to improve the Naive Bayes classifier from last class using techniques we learned today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this every time you open the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import Counter\n",
    "from lib import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title lib.py { display-mode: \"form\" }\n",
    "\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import pandas \n",
    "import codecs\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "SMOOTH_CONST = 0.001 # we want this to be smaller than 1/n where n is the size of the largest training category. that way, any word that has appeared exactly once (with category c) in training will still have a larger probability for category c, than any other category c'\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "categories = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
    "need_or_resource_labels = ['need', 'resource', 'N/A']\n",
    "\n",
    "\n",
    "class Tweet(object):\n",
    "  def __init__(self, tweetSurfaceForm, category, need_or_resource):\n",
    "    if isinstance(tweetSurfaceForm, str):\n",
    "      self.tokenList = word_tokenize(tweetSurfaceForm)\n",
    "    else:\n",
    "      self.tokenList = word_tokenize(tweetSurfaceForm.decode('utf-8','ignore'))\n",
    "    self.tokenList = [t.lower() for t in self.tokenList] # lowercase\n",
    "    self.tokenSet = set(self.tokenList)\n",
    "    self._bigramList = [(self.tokenList[idx], self.tokenList[idx+1]) for idx in range(len(self.tokenList)-1)]\n",
    "    self._featureSet = set(self._bigramList).union(self.tokenSet)\n",
    "    self.category = category\n",
    "    self.need_or_resource = need_or_resource\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    return self.tokenList[index]\n",
    "\n",
    "  def idx(self, token):\n",
    "    return self.tokenList.index(token)\n",
    "\n",
    "  def __unicode__(self):\n",
    "    return \" \".join(self.tokenList)\n",
    "\n",
    "  def __str__(self):\n",
    "    # return unicode(self).encode('utf-8')\n",
    "    return \" \".join(self.tokenList)\n",
    "\n",
    "  def __repr__(self):\n",
    "      return self.__str__()\n",
    "\n",
    "def preprocess_tweet(tweet, stop_words=[], stemmer=None, lmtzr=None): \n",
    "    assert stemmer is None or lmtzr is None, 'Stemmer and lemmatizer cannot be provided at the same time. Please choose at most one.'\n",
    "    new_tokenList = []\n",
    "    for token in tweet.tokenList: \n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        if stemmer is not None:\n",
    "          token = stemmer.stem(token)\n",
    "        elif lmtzr is not None:\n",
    "          token = lmtzr.lemmatize(token)\n",
    "        new_tokenList.append(token)\n",
    "    tweet.tokenList = new_tokenList\n",
    "    tweet.tokenSet = set(tweet.tokenList)\n",
    "    tweet._bigramList = [(tweet.tokenList[idx], tweet.tokenList[idx+1]) for idx in range(len(tweet.tokenList)-1)]\n",
    "    tweet._featureSet = set(tweet._bigramList).union(tweet.tokenSet)\n",
    "    return tweet\n",
    "    \n",
    "def read_csv(path):\n",
    "   data = {}\n",
    "   with open(path) as f:\n",
    "     reader = csv.reader(f)\n",
    "     for row in reader:\n",
    "       (tweetId, tweetText, category, need_or_resource) = row\n",
    "       assert category in categories\n",
    "       assert need_or_resource in need_or_resource_labels\n",
    "       if need_or_resource == \"N/A\":\n",
    "         assert category == \"None\"\n",
    "       #assert tweetId not in data.keys()\n",
    "       data[tweetId] = Tweet(tweetText, category, need_or_resource)\n",
    "   data = list(data.values()) # list of Tweets\n",
    "   return data\n",
    "\n",
    "def read_github_data(path): \n",
    "  df = pandas.read_csv(path)\n",
    "  data = []\n",
    "  for i in range(len(df)): \n",
    "    tweetText = df[\"Text\"][i]\n",
    "    category  = df[\"Class\"][i]\n",
    "    need_or_resource_labels = df[\"Type\"][i]\n",
    "    tweet = Tweet(tweetText, category, need_or_resource_labels)\n",
    "    data.append(tweet)\n",
    "  return data\n",
    "\n",
    "\n",
    "def read_data(train_path = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_labeled-data-singlelabels-train.csv',\n",
    "                    test_path = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_labeled-data-singlelabels-test.csv'):\n",
    "  \"\"\"Returns two lists of tweets: the train set and the test set\"\"\"\n",
    "  train_tweets = read_github_data(train_path)\n",
    "  test_tweets = read_github_data(test_path)\n",
    "  return train_tweets, test_tweets\n",
    "\n",
    "def read_sandy_data(train_path = 'data/labeled-data-singlelabels-train.csv',\n",
    "                    test_path = 'data/labeled-data-singlelabels-test.csv'):\n",
    "  \"\"\"Returns two lists of tweets: the train set and the test set\"\"\"\n",
    "  return read_data(train_path, test_path)\n",
    "\n",
    "def read_haiti_data(train_path = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_haiti_train.csv',\n",
    "                    test_path = 'https://raw.githubusercontent.com/eliaszwang/AI4ALL2020/master/data/pd_haiti_test.csv'):\n",
    "  \"\"\"Returns two lists of tweets: the train set and the test set\"\"\"\n",
    "  train_tweets = read_github_data(train_path)\n",
    "  test_tweets = read_github_data(test_path)\n",
    "  return train_tweets, test_tweets\n",
    "\n",
    "# def read_haiti_data(train_path = 'data/haiti_train.csv',\n",
    "#                     test_path = 'data/haiti_test.csv'):\n",
    "#   \"\"\"Returns two lists of tweets: the train set and the test set\"\"\"\n",
    "#   return read_data(train_path, test_path)\n",
    "\n",
    "\n",
    "# def show_confusion_matrix(predictions):\n",
    "#   \"\"\"Displays a confusion matrix as a HTML table.\n",
    "#   Rows are true label, columns are predicted label.\n",
    "#   predictions is a list of (tweet, predicted_category) pairs\"\"\"\n",
    "#   num_categories = len(categories)\n",
    "#   conf_mat = np.zeros((num_categories, num_categories), dtype=np.int32)\n",
    "#   for (tweet,predicted_category) in predictions:\n",
    "#     gold_idx = categories.index(tweet.category)\n",
    "#     predicted_idx = categories.index(predicted_category)\n",
    "#     conf_mat[gold_idx, predicted_idx] += 1\n",
    "#   df = pandas.DataFrame(data=conf_mat, columns=categories, index=categories)\n",
    "#   display(HTML(df.to_html()))\n",
    "\n",
    "def show_confusion_matrix(predictions):\n",
    "  conf_mat = calc_confusion_matrix(predictions)\n",
    "  disp_confusion_matrix(conf_mat)\n",
    "  return conf_mat\n",
    "\n",
    "def calc_confusion_matrix(predictions):\n",
    "  num_categories = len(categories)\n",
    "  conf_mat = np.zeros((num_categories, num_categories), dtype=np.int32)\n",
    "  for (tweet,predicted_category) in predictions:\n",
    "    gold_idx = categories.index(tweet.category)\n",
    "    predicted_idx = categories.index(predicted_category)\n",
    "    conf_mat[gold_idx, predicted_idx] += 1\n",
    "  return conf_mat\n",
    "\n",
    "def disp_confusion_matrix(conf_mat):\n",
    "  df = pandas.DataFrame(data=conf_mat, columns=categories, index=categories)\n",
    "  display(HTML(df.to_html()))  \n",
    "\n",
    "  n_rows, n_cols = conf_mat.shape\n",
    "  for r in range(n_rows):\n",
    "    cnt = np.sum(conf_mat[r])\n",
    "    for c in range(n_cols):\n",
    "      conf_mat[r,c] *= 256 / cnt\n",
    "\n",
    "  plt.imshow(conf_mat, cmap=plt.cm.Blues)\n",
    "\n",
    "\n",
    "def class_pie_chart(tweets):\n",
    "  tweet_categories = [tweet.category for tweet in tweets]\n",
    "  tweet_cnt = Counter(tweet_categories)\n",
    "  # total_cnt = sum(tweet_cnt.values())\n",
    "  sizes, labels = [], []\n",
    "  for category in sorted(tweet_cnt.keys()):\n",
    "    sizes.append(tweet_cnt[category])\n",
    "    labels.append(category)\n",
    "  colors = ['yellowgreen', 'gold', 'lightcoral', 'gray', 'lightskyblue']\n",
    "  plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "  plt.axis('equal')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def metric_bar_chart(categories, precisions, recalls, f1s):\n",
    "  \"\"\"\n",
    "  results:\n",
    "    - keys = categories\n",
    "    - each value: a list of 3 numbers: recall, precision, f1\n",
    "  \"\"\"\n",
    "  # create plot\n",
    "  fig, ax = plt.subplots()\n",
    "  index = np.arange(len(categories))\n",
    "  bar_width = 0.2\n",
    "  opacity = 0.8\n",
    "   \n",
    "  precision_bar = plt.bar(index, precisions, bar_width,\n",
    "                   alpha=opacity,\n",
    "                   color='yellowgreen',\n",
    "                   label='Precision')\n",
    "   \n",
    "  recall_bar = plt.bar(index + bar_width, recalls, bar_width,\n",
    "                   alpha=opacity,\n",
    "                   color='gold',\n",
    "                   label='Recall')\n",
    "  f1_bar = plt.bar(index + 2*bar_width, f1s, bar_width,\n",
    "                   alpha=opacity,\n",
    "                   color='lightskyblue',\n",
    "                   label='F1')\n",
    "   \n",
    "  plt.xlabel('Categories')\n",
    "  plt.ylabel('Metrics')\n",
    "  plt.title('Performance by Category')\n",
    "  plt.xticks(index + 1.5*bar_width, categories)\n",
    "  plt.legend()\n",
    "   \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def class2color_style(s):\n",
    "  class2color = {\n",
    "    'Energy' : 'red',\n",
    "    'Food': 'orange',\n",
    "    'Medical': 'green',\n",
    "    'None': 'gray',\n",
    "    'Water': 'blue',\n",
    "    'resource': 'purple',\n",
    "    'need': 'pink',\n",
    "    'N/A': 'gray',\n",
    "  }\n",
    "  try:\n",
    "    return \"color: %s\" % class2color[s]\n",
    "  except KeyError:\n",
    "    return \"color: black\"\n",
    "\n",
    "\n",
    "def show_tweets(tweets, search_term=None):\n",
    "  \"\"\"Displays a HTML table of tweets alongside labels\"\"\"\n",
    "  if search_term is not None:\n",
    "    tweets = [t for t in tweets if search_term in str(t).lower()]\n",
    "  columns = ['Text', 'Category', 'Need or resource']\n",
    "  data = [[str(t), t.category, t.need_or_resource] for t in tweets]\n",
    "  pandas.set_option('display.max_colwidth', -1)\n",
    "  df = pandas.DataFrame(data, columns=columns)\n",
    "  s = df.style.applymap(class2color_style)\\\n",
    "              .set_properties(**{'text-align': 'left'})\n",
    "  display(HTML(s.render()))\n",
    "\n",
    "\n",
    "def show_predictions(predictions, show_mistakes_only=False):\n",
    "  \"\"\"Displays a HTML table comparing true categories to predicted categories.\n",
    "  predictions is a list of (tweet, predicted_category) pairs\"\"\"\n",
    "  if show_mistakes_only:\n",
    "    predictions = [(t,p) for (t,p) in predictions if t.category!=p]\n",
    "  columns = ['Text', 'True category', 'Predicted category']\n",
    "  data = [[str(t), t.category, predicted_category] for (t,predicted_category) in predictions]\n",
    "  pandas.set_option('display.max_colwidth', -1)\n",
    "  df = pandas.DataFrame(data, columns=columns)\n",
    "  s = df.style.applymap(class2color_style)\\\n",
    "              .set_properties(**{'text-align': 'left'})\n",
    "  display(HTML(s.render()))\n",
    "\n",
    "\n",
    "\n",
    "def most_discriminative(tweets, token_probs, prior_probs):\n",
    "  \"\"\"Prints, for each category, which tokens are most discriminative i.e. maximize P(category|token), including normalization by P(token)\"\"\"\n",
    "  all_tokens = set([token for tweet in tweets for token in tweet.tokenSet])\n",
    "\n",
    "  token2dist = {} # maps token to a probability distribution over categories, for a tweet containing just this token\n",
    "\n",
    "  for token in all_tokens:\n",
    "    single_token_tweet = Tweet(token, \"\", \"\")\n",
    "    log_dist = {c: get_log_posterior_prob(single_token_tweet, prior_probs[c], token_probs[c]) for c in categories}\n",
    "    min_log_dist = min(log_dist.values())\n",
    "    log_dist = {c: l+min_log_dist for c,l in log_dist.items()} # shift so smallest value is 0 before taking exp\n",
    "    dist = {c:math.exp(l) for c,l in log_dist.items()} # take exp\n",
    "    s = sum(dist.values())\n",
    "    dist = {c: dist[c]/s for c in categories} # normalize\n",
    "    token2dist[token] = dist\n",
    "\n",
    "  # for each category print the tokens that maximize P(C|token) (normalized by P(token))\n",
    "  print(\"MOST DISCRIMINATIVE TOKENS: \\n\")\n",
    "  for c in categories:\n",
    "    probs = [(token,dist[c]) for token,dist in token2dist.items()]\n",
    "    probs = sorted(probs, key=lambda x: x[1], reverse=True)\n",
    "    print(\"{0:20} {1:10}\".format(\"TOKEN\", \"P(%s|token)\"%c))\n",
    "    for (token,p) in probs[:10]:\n",
    "        print(\"{0:20} {1:.4f}\".format(str(token.encode('utf-8')), p))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def get_category_f1(predictions, c):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "      predictions: a list of (tweet, predicted_category) pairs\n",
    "      c: a category\n",
    "  Calculate the precision, recall and F1 for a single category c (e.g. Food)\n",
    "  \"\"\"\n",
    "\n",
    "  true_positives = 0.0\n",
    "  false_positives = 0.0\n",
    "  false_negatives = 0.0\n",
    "\n",
    "  for (tweet, predicted_category) in predictions:\n",
    "      true_category = tweet.category\n",
    "      if true_category == c and predicted_category == c:\n",
    "          true_positives += 1\n",
    "      elif true_category == c and predicted_category != c:\n",
    "          false_negatives += 1\n",
    "      elif true_category != c and predicted_category == c:\n",
    "          false_positives += 1\n",
    "\n",
    "  if true_positives == 0:\n",
    "      precision = 0.0\n",
    "      recall = 0.0\n",
    "      f1 = 0.0\n",
    "  else:\n",
    "      precision = true_positives*100 / (true_positives + false_positives)\n",
    "      recall = true_positives*100 / (true_positives + false_negatives)\n",
    "      f1 = 2*precision*recall / (precision + recall)\n",
    "\n",
    "  print(c)\n",
    "  print(\"Precision: \", precision)\n",
    "  print(\"Recall: \", recall)\n",
    "  print(\"F1: \", f1)\n",
    "  print(\"\")\n",
    "#     p(rint \"Class %s: precision %.2f, recall %.2f, F1 %.2f\" % (c, precision, recall, f1)\n",
    "\n",
    "  return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate(predictions, has_return=False):\n",
    "  \"\"\"Calculate average F1\"\"\"\n",
    "  average_f1 = 0\n",
    "  precisions, recalls, f1s = [], [], []\n",
    "  for c in categories:\n",
    "    precision, recall, f1 = get_category_f1(predictions, c)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    average_f1 += f1\n",
    "\n",
    "  average_f1 /= len(categories)\n",
    "  print(\"Average F1: \", average_f1)\n",
    "  if has_return:\n",
    "    return categories, precisions, recalls, f1s\n",
    "\n",
    "\n",
    "def calc_probs(tweets, c):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        c: a string representing a category\n",
    "    Returns:\n",
    "        prob_c: the prior probability of category c\n",
    "        feature_probs: a Counter mapping each feature to P(feature|category c)\n",
    "    \"\"\"\n",
    "    num_tweets = len(tweets)\n",
    "    num_tweets_about_c = len([t for t in tweets if t.category==c])\n",
    "    prob_c = float(num_tweets_about_c)/num_tweets\n",
    "    feature_counts = Counter() # maps token -> count and bigram -> count\n",
    "    for tweet in tweets:\n",
    "        if tweet.category==c:\n",
    "          for feature in tweet._featureSet:\n",
    "            feature_counts[feature] += 1\n",
    "    feature_probs = Counter({feature: float(count)/num_tweets_about_c for feature,count in feature_counts.items()})\n",
    "    return prob_c, feature_probs\n",
    "\n",
    "\n",
    "def calc_probs_single(tweets, c, stop_words=[], stemmer=None, lmtzr=None):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        c: a string representing a category; one of \"Energy\", \"Food\", \"Medical\", \"Water\", \"None\". \n",
    "    Returns:\n",
    "        prob_c: the prior probability of category c\n",
    "        token_probs: a Counter mapping each token to P(token|category c)\n",
    "    \"\"\"\n",
    "    assert stemmer is None or lmtzr is None, 'Stemmer and lemmatizer cannot be provided at the same time. Please choose at most one.'\n",
    "    num_tweets = len(tweets)\n",
    "    num_tweets_about_c = sum([tweet.category == c for tweet in tweets])\n",
    "    \n",
    "    prob_c = num_tweets_about_c / num_tweets\n",
    "    \n",
    "    token_counts = Counter()\n",
    "    for tweet in tweets:\n",
    "        # print(tweet.category)\n",
    "        if tweet.category == c:\n",
    "            for token in tweet.tokenSet:\n",
    "                if token in stop_words:\n",
    "                    continue\n",
    "                if stemmer is not None:\n",
    "                  token = stemmer.stem(token)\n",
    "                elif lmtzr is not None:\n",
    "                  token = lmtzr.lemmatize(token)\n",
    "                token_counts[token] += 1\n",
    "\n",
    "    token_probs = Counter()\n",
    "    for token in token_counts:\n",
    "        token_probs[token] = token_counts[token] / num_tweets_about_c\n",
    "    \n",
    "    return prob_c, token_probs\n",
    "\n",
    "def get_posterior_prob_single(tweet, prob_c, token_probs, stop_words=[], stemmer=None, lmtzr=None, unseen_prob=SMOOTH_CONST):\n",
    "    \"\"\"Calculate the posterior P(c|tweet). \n",
    "    (Actually, calculate something proportional to it).\n",
    "    \n",
    "    Inputs:\n",
    "        tweet: a tweet\n",
    "        prob_c: the prior probability of category c\n",
    "        token_probs: a Counter mapping each token P(token|c)\n",
    "    Return:\n",
    "        The posterior P(c|tweet).\n",
    "    \"\"\"\n",
    "    assert stemmer is None or lmtzr is None, 'Stemmer and lemmatizer cannot be provided at the same time. Please choose at most one.'\n",
    "\n",
    "    posterior = np.log(prob_c)\n",
    "    \n",
    "    for token in tweet.tokenSet:\n",
    "        if token in stop_words:\n",
    "          continue\n",
    "\n",
    "        if stemmer is not None:\n",
    "          token = stemmer.stem(token)\n",
    "        elif lmtzr is not None:\n",
    "          token = lmtzr.lemmatize(token)\n",
    "\n",
    "        prob = token_probs[token]\n",
    "        if prob == 0:\n",
    "          prob = unseen_prob\n",
    "        posterior += np.log(prob)\n",
    "\n",
    "    return posterior\n",
    "\n",
    "def classify_nb_single(tweet, probs, stop_words=[], stemmer=None, lmtzr=None):\n",
    "    \"\"\"Classifies a tweet. Calculates the posterior P(c|tweet) for each category c, \n",
    "    and returns the category with largest posterior.\n",
    "    Input:\n",
    "        tweet\n",
    "    Output:\n",
    "        string equal to most-likely category for this tweet\n",
    "    \"\"\"\n",
    "    assert stemmer is None or lmtzr is None, 'Stemmer and lemmatizer cannot be provided at the same time. Please choose at most one.'\n",
    "\n",
    "    cat_probs = []\n",
    "    for category in probs:\n",
    "      prior_prob, token_prob = probs[category]\n",
    "      posterior = get_posterior_prob_single(tweet, prior_prob, token_prob, stop_words, stemmer=stemmer, lmtzr=lmtzr)\n",
    "      cat_probs.append((category, posterior))\n",
    "\n",
    "    sorted_classes = sorted(cat_probs, key=lambda x:x[1])\n",
    "    max_class = sorted(cat_probs, key=lambda x:x[1])[-1][0]\n",
    "    return max_class\n",
    "\n",
    "\n",
    "def learn_nb(tweets):\n",
    "  feature_probs = {}\n",
    "  prior_probs = {}\n",
    "  for c in categories:\n",
    "    prior_c, feature_probs_c = calc_probs(tweets, c)\n",
    "    feature_probs[c] = feature_probs_c\n",
    "    prior_probs[c] = prior_c\n",
    "  return prior_probs, feature_probs\n",
    "\n",
    "\n",
    "def get_log_posterior_prob(tweet, prob_c, feature_probs_c):\n",
    "    \"\"\"Calculate the posterior P(c|tweet).\n",
    "    (Actually, calculate something proportional to it).\n",
    "\n",
    "    Inputs:\n",
    "        tweet: a tweet\n",
    "        prob_c: the prior probability of category c\n",
    "        feature_probs_c: a Counter mapping each feature to P(feature|c)\n",
    "    Return:\n",
    "        The posterior P(c|tweet).\n",
    "    \"\"\"\n",
    "    log_posterior = math.log(prob_c)\n",
    "    for feature in tweet._featureSet:\n",
    "        if feature_probs_c[feature] == 0:\n",
    "            log_posterior += math.log(SMOOTH_CONST)\n",
    "        else:\n",
    "            log_posterior += math.log(feature_probs_c[feature])\n",
    "    return log_posterior\n",
    "\n",
    "\n",
    "def classify_nb(tweet, prior_probs, token_probs, rule_based=False):\n",
    "    \"\"\"Classifies a tweet. Calculates the posterior P(c|tweet) for each category c,\n",
    "    and returns the category with largest posterior.\n",
    "    Input:\n",
    "        tweet\n",
    "    Output:\n",
    "        string equal to most-likely category for this tweet\n",
    "    \"\"\"\n",
    "    if rule_based:\n",
    "      matches = set(['energy', 'electricity', 'electrical', 'generator', 'generators', 'blackout', 'power'])\n",
    "      for token in tweet.tokenSet:\n",
    "        if token.lower() in matches:\n",
    "          return 'Energy'\n",
    "    log_posteriors = {c: get_log_posterior_prob(tweet, prior_probs[c], token_probs[c]) for c in categories}\n",
    "    return max(log_posteriors.keys(), key=lambda c:log_posteriors[c])\n",
    "\n",
    "\n",
    "def get_box_contents(n_boxes = 2):\n",
    "    box1 = [\"red\"] * 10 + [\"blue\"] * 39 + [\"yellow\"] * 1 + [\"green\"] * 27 + [\"orange\"] * 23\n",
    "    box2 = [\"red\"] * 53 + [\"blue\"] * 5 + [\"yellow\"] * 25 + [\"green\"] * 9 + [\"orange\"] * 8\n",
    "    box3 = [\"red\"] * 15 + [\"blue\"] * 15 + [\"yellow\"] * 64 + [\"green\"] * 3 + [\"orange\"] * 3\n",
    "    box4 = [\"red\"] * 5 + [\"blue\"] * 5 + [\"yellow\"] * 5 + [\"green\"] * 5 + [\"orange\"] * 80\n",
    "\n",
    "\n",
    "    assert(len(box1) == 100)\n",
    "    assert(len(box2) == 100)\n",
    "    assert(len(box3) == 100)\n",
    "    assert(len(box4) == 100)\n",
    "\n",
    "\n",
    "    random.shuffle(box1)\n",
    "    random.shuffle(box2)\n",
    "    random.shuffle(box3)\n",
    "    random.shuffle(box4)\n",
    "\n",
    "    boxes = [box1, box2, box3, box4][0:n_boxes]\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_tweet(tweet, prior_probs, token_probs):\n",
    "    \"\"\"\n",
    "        Visualizes a tweet and its probabilities in an IPython notebook.\n",
    "        Input:\n",
    "            tweet: a tweet as a string\n",
    "            prior_probs: priors for each category\n",
    "            token_probs: a dictionary of Counters that contain the unigram\n",
    "               probabilities for each category\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # boileplate HTML part 1\n",
    "    html = \"\"\"\n",
    "    <div id=\"viz-overlay\" style=\"display:none;position:absolute;width:250px;height:110px;border: 1px solid #000; padding:8px;  background: #eee;\">\n",
    "\t<p>\n",
    "       <span style=\"color:orange;\">P(<span class=\"viz-token-placeholder\"></span> | food) = <span id=\"viz-p-food\"></span></span><br>\n",
    "\t   <span style=\"color:blue;\">P(<span class=\"viz-token-placeholder\"></span> | water) = <span id=\"viz-p-water\"></span><br>\n",
    "\t   <span style=\"color:green;\">P(<span class=\"viz-token-placeholder\"></span> | medical) = <span id=\"viz-p-medical\"></span><br>\n",
    "\t   <span style=\"color:red;\">P(<span class=\"viz-token-placeholder\"></span> | energy) = <span id=\"viz-p-energy\"></span><br>\n",
    "\t   <span style=\"color:gray;\">P(<span class=\"viz-token-placeholder\"></span> | none) = <span id=\"viz-p-none\"></span></p>\n",
    "    </p>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"viz-tweet\" style=\"padding: 190px 0 0;\">\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tokens = tweet.tokenList\n",
    "    categories = [\"None\", \"Food\", \"Medical\", \"Energy\", \"Water\"]\n",
    "    for token in tokens:\n",
    "        probs = [token_probs['None'][token], token_probs['Food'][token],\n",
    "                token_probs['Medical'][token], token_probs['Energy'][token],\n",
    "                token_probs['Water'][token]]\n",
    "        idx = np.argmax(probs) if sum(probs) > 0 else 0\n",
    "        max_class = categories[idx]\n",
    "\n",
    "        html += '<span style=\"%s\" class=\"viz-token\" data-food=\"%f\" data-none=\"%f\" data-medical=\"%f\" data-energy=\"%f\" data-water=\"%f\">%s</span> ' \\\n",
    "                  % (class2color_style(max_class), token_probs['Food'][token], token_probs['None'][token], token_probs['Medical'][token],\n",
    "                  token_probs['Energy'][token], token_probs['Water'][token], token)\n",
    "\n",
    "    # Predicted category.\n",
    "    predicted_category = classify_nb(tweet, prior_probs, token_probs)\n",
    "    html += '<p><strong>Predicted category: </strong> <span style=\"%s\"> %s</span><br>' \\\n",
    "              % (class2color_style(predicted_category), predicted_category)\n",
    "    html += '<strong>True category: </strong> <span style=\"%s\"> %s</span></p>' \\\n",
    "              % (class2color_style(tweet.category), tweet.category)\n",
    "\n",
    "    #Javascript\n",
    "    html += \"\"\"\n",
    "    </div>\n",
    "     <script type=\"text/javascript\">\n",
    "\t$(document).ready(function() {\n",
    "\t\t$(\"span.viz-token\").mouseover(function() {\n",
    "\t\t\t$(\"span.viz-token\").css({\"font-weight\": \"normal\"});\n",
    "\t\t\t$(this).css({\"font-weight\": \"bold\"});\n",
    "\t\t\t$(\"span.viz-token-placeholder\").text($(this).text());\n",
    "\t\t\t$(\"#viz-p-food\").text($(this).data(\"food\"));\n",
    "\t\t\t$(\"#viz-p-water\").text($(this).data(\"water\"));\n",
    "\t\t\t$(\"#viz-p-medical\").text($(this).data(\"medical\"));\n",
    "\t\t\t$(\"#viz-p-energy\").text($(this).data(\"energy\"));\n",
    "\t\t\t$(\"#viz-p-none\").text($(this).data(\"none\"));\n",
    "\t\t\t$(\"#viz-overlay\").show();\n",
    "\t\t\t$(\"#viz-overlay\").offset({left:$(this).offset().left-110+$(this).width()/2, top:$(this).offset().top - 140});\n",
    "\t\t});\n",
    "\t});\n",
    "    </script>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below commands to import modules and download datasets that we will use for our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.corpus import reuters, movie_reviews, shakespeare\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('reuters')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('shakespeare')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a corpus: reuters, movie_reviews or shakespeare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = movie_reviews # Choose by editing here.\n",
    "\n",
    "if corpus==shakespeare:\n",
    "    shakespeare_text = ''.join([''.join(corpus.xml(fileid).itertext()) for fileid in corpus.fileids()])\n",
    "    words = word_tokenize(shakespeare_text)\n",
    "    sents = [word_tokenize(sent) for sent in sent_tokenize(shakespeare_text)]\n",
    "else:    \n",
    "    words = corpus.words()\n",
    "    sents = corpus.sents()\n",
    "\n",
    "# Lowercase everything\n",
    "words = [w.lower() for w in words] # List of words in corpus.\n",
    "sents = [[w.lower() for w in sent] for sent in sents] # List of sentences in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out a few words and sentences in `words` and `sents`. See if you can find a funny sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the total number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram language model\n",
    "\n",
    "In this section, we will construct a language model based on unigrams (words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Make a Counter from the list of words and call it `unigram_count`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Get the total number of words and assign it to `total_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####\n",
    "\n",
    "print(\"Total number of words in corpus: \", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to find the 10 most common words in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 most common words\n",
    "print(\"\\nTop 10 most common words: \")\n",
    "for (word, count) in unigram_counts.most_common(n=10):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the Counter unigram_counts, which maps each word to its count.  We want to construct the Counter unigram_probs, which maps each word to its probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Create an empty Counter called `unigram_probs`. Using a for-loop over `unigram_counts`, (this will iterate over the keys i.e. words) calculate the appropriate fraction, and add the word -> fraction pair to `unigram_probs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.) Check that the probabilities in `unigram_probs` add up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####\n",
    "\n",
    "print(\"Probabilities sum to: \", probabiltity_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the probability of word \"the\", then try some other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to write code in this cell, just run it to generate 100 words of language using the unigram model. Run this cell several times! What do you observe about the unigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [] # will be a list of generated words\n",
    "\n",
    "for _ in range(100):\n",
    "    r = random.random() # random number in [0,1]\n",
    "    \n",
    "    # Find the word whose \"interval\" contains r\n",
    "    accumulator = .0\n",
    "    for word, freq in unigram_probs.items():\n",
    "        accumulator += freq\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram language model\n",
    "\n",
    "In this section, we'll build a language model based on bigrams (pairs of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to write code in the cell below, just run it! :) This code counts how often each bigram occurs and uses a bigram utility function from nltk.\n",
    "\n",
    "`bigram_counts` is a dictionary that maps w1 to a dictionary mapping w2 to the count for (w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = defaultdict(lambda: Counter())\n",
    "\n",
    "for sentence in sents:\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        bigram_counts[w1][w2] += 1\n",
    "        \n",
    "bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print how often the bigram \"of the\" occurs. Try some other words following \"of\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigram_counts['of']['the'])\n",
    "\n",
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to write code in the cell below, just run it! :) The code transforms the bigram counts to bigram probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs = defaultdict(lambda: Counter())\n",
    "for w1 in bigram_counts:\n",
    "    total_count = float(sum(bigram_counts[w1].values()))\n",
    "    bigram_probs[w1] = Counter({w2: c/total_count for w2,c in bigram_counts[w1].items()})\n",
    "    \n",
    "# Print the probability that 'the' follows 'of'\n",
    "print(bigram_probs['of']['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints the top ten tokens most likely to follow 'fair', along with their probabilities. Try some other words! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dist = bigram_probs['fair']\n",
    "for word,prob in prob_dist.most_common(10):\n",
    "    print(word,\"{:.5f}\".format(prob))\n",
    "    \n",
    "# Try some other words.\n",
    "### YOUR CODE HERE ####\n",
    "\n",
    "### END CODE HERE ####\n",
    "\n",
    "print(word, prob_dist[word]) # This prints the probability of your word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to write code in this cell, just run it to generate text using the bigram model. Run this cell several times! How does the bigram text compare to the unigram text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to write code in this cell, just run it! :)\n",
    "# Generate text with bigram model.\n",
    "# Run this cell several times!\n",
    "\n",
    "text = [None] # You can put your own starting word in here\n",
    "sentence_finished = False\n",
    "\n",
    "# Generate words until a None is generated\n",
    "while not sentence_finished:\n",
    "    r = random.random() # random number in [0,1]\n",
    "    accumulator = .0\n",
    "    latest_token = text[-1]\n",
    "    prob_dist = bigram_probs[latest_token] # prob dist of what token comes next\n",
    "    \n",
    "    # Find the word whose \"interval\" contains the random number r.\n",
    "    for word,p in prob_dist.items():\n",
    "        accumulator += p \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-1] == None:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing the Results of our Naive Bayes Classifer\n",
    "\n",
    "In preparation for the mini-projects today, we're going to look at some tools to analyze the results of the Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. This function returns `tweets` and `test_tweet`, both lists of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, test_tweets = lib.read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous notebooks, we have implemented a Naive Bayes classifier on the data. Let's remind of ourselves about how well it performs. \n",
    "\n",
    "*Note: this notebook uses our implementation of the Naive Bayes classifier, but it's very similar to what you implemented yesterday. If you're interested in the details, take a look at the `evaluate_nb_single`, `learn_nb` and `classify_nb` functions in `lib.py` collapsed code block at the top of this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lib.evaluate_nb_single(tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our Naive Bayes classifier scores an Average F1 score of 66.9% on the test set. Let's see if we can improve on that by trying out some techniques we learned today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Inspecting the Classifier\n",
    "\n",
    "After implementing and training a classifier, you often want to inspect what kind of things it has learned and how it is making predictions on individual examples. This can help you make sure that you implemented everything correctly and it might give you ideas on how to further improve the classifier.\n",
    "\n",
    "### Most discriminative words\n",
    "\n",
    "Let's first look again at the most discriminative words for each category, i.e. the words that maximize P(category|word), for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.most_discriminative(tweets, results[\"token_probs\"], results[\"prior_probs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These five lists show you which words are most predictive of the five categories. For example, the word _bottled_ is a very strong indicator that the tweet is about water or the word _canned_ is a very strong indicator that the tweet is about food.\n",
    "\n",
    "Many of you used several of these words in your rule-based classifiers in week 1. It's reassuring (and exciting!) to see that the Naive Bayes classifier learned that these words are good indicators of the categories as well.\n",
    "\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "Another useful type of visualization is the confusion matrix which we covered a bit when talking about Evaluation. A confusion matrix shows you for each true category _c_ how many of the tweets in _c_ were classified into the five different categories. (In this way it tells you which categories are \"confused\" for others by the classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.show_confusion_matrix(results[\"predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix, the **rows** correspond to the **true category** and the **columns** correspond to the **predicted category**.\n",
    "\n",
    "### Visualizing individual tweets\n",
    "\n",
    "It can also be really useful to visualize the probabilities of each token in an individual tweet. This can help you understand why a classifier made a correct or wrong prediction. We've implemented a visualization for you so that you can use to inspect how the classifier works on individual tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code visualizes a random tweet from the test data. \n",
    "# Hover your mouse over the words!\n",
    "import random\n",
    "random_tweet = random.choice(test_tweets)\n",
    "lib.visualize_tweet(random_tweet, results[\"prior_probs\"], results[\"token_probs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color of each word tells you for which category $P(\\text{token} \\mid \\text{category})$ is the highest. When you move the mouse over a word, it shows you the actual values of $P(\\text{token} \\mid \\text{category})$ for each category that the classifier uses to make its predictions.\n",
    "\n",
    "You can also have the classifier make a prediction on your own tweets. Change the text in `my_tweet` below and run the cell below to see what the classifier would predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = \"I urgently need some bottled water.\"\n",
    "\n",
    "lib.visualize_tweet(lib.Tweet(my_tweet, \"?\", \"\"), results[\"prior_probs\"], results[\"token_probs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Group Mini-Projects \n",
    "\n",
    "Now we are ready to do the mini-projects! We're going to divide into two groups and each group will be completing one of the mini-projects below to try to improve our Naive Bayes classifier. \n",
    "\n",
    "Then, we'll regroup and teach each other about what we found! \n",
    "\n",
    "\n",
    "### (1) Using Preprocessing to Improve Our Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can improve performance by applying preprocessing techniques. \n",
    "\n",
    "Some questions you may have:\n",
    "\n",
    "1. Are all words equally informative?\n",
    "2. Words such as \"generator\" and \"generators\" seem to convey the same meaning. Can we merge them?\n",
    "\n",
    "Next, we are going to play with three pre-processing steps to address these two questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "\n",
    "Stop words, or function words (as opposed to content words), refer to commonly used words that are usually non-informative, such as \"the\", \"a\", or \"can\".\n",
    "\n",
    "It is usually advantageous for the classifier to ignore these stop words, since they may add noises or cause numerical issues (e.g. underflow).\n",
    "\n",
    "The nltk package provides a list of stop words in English, and we can remove them from our data simply by using equality tests, which can be considered as a rule-based classifier that classifies whether a word is a stop word or not by looking up a blacklist (i.e. the list of stop words).\n",
    "\n",
    "Let's first look at some examples of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "# look at some stopwords\n",
    "print(\"Here are some example stopwords:\")\n",
    "for i,word in enumerate(eng_stopwords):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Create a list called `deleted_tokens` which includes all tokens that are stopwords in the tweets and a list called `filtered_tokens` which includes all tokens that are NOT stopwords in the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, test_tweets = lib.read_data()\n",
    "\n",
    "tweet = tweets[0]\n",
    "tokens = tweet.tokenSet\n",
    "print('all tokens:\\n', tokens, '\\n')\n",
    "\n",
    "filtered_tokens = set()\n",
    "deleted_tokens = set()\n",
    "\n",
    "for token in tweet.tokenSet:\n",
    "    \n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    if token in eng_stopwords:\n",
    "        deleted_tokens.add(token)\n",
    "    else:\n",
    "        filtered_tokens.add(token)\n",
    "    ##### YOUR CODE ENDS HERE #####\n",
    "        \n",
    "print('filtered_tokens:\\n', filtered_tokens, '\\n')\n",
    "print('deleted_tokens:\\n', deleted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Now let's see if removing stop words actually helps with the classification performance. No need to add code to the cell below just run it! :)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.evaluate_nb_single(tweets, test_tweets, stop_words=eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these results with the previous ones. Does stop word removal help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "\n",
    "Remember that the goal of stemming is to reduce different word forms to a common base form (i.e. stem). The powerful `nltk` provides tools we can use for stemming. \n",
    "\n",
    "##### Stemming using the Porter stemmer\n",
    "*Porter's algorithm*, developed in the 1980s, is one of the most commonly used stemmers.\n",
    "\n",
    "3.) Run the cell below to see how different words are stemmed. Add your own words to the lists to see how they are stemmed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "# Get the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Let's stemming on plurals\n",
    "plurals = ['apples', 'batteries', 'generators', 'medicines', 'tests', 'feet']\n",
    "print('plurals:')\n",
    "for plural in plurals:\n",
    "    print('{:s} --> {:s}'.format(plural, stemmer.stem(plural)))\n",
    "print()\n",
    "    \n",
    "# and variations of verbs\n",
    "verbs = ['studies', 'thinks', 'goes', 'played', 'bought', 'went', 'ran', 'drew', ]\n",
    "print('verbs:')\n",
    "for verb in verbs:\n",
    "    print('{:s} --> {:s}'.format(verb, stemmer.stem(verb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that the results may look a bit mechanical. This is because the Porter's algorithm is essentially a sequential application of a set of rules. \n",
    "\n",
    "4.) Let's check whether stemming can help with our classification task. How does it do? Run the cell below to evaluate Naive Bayes with stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "lib.evaluate_nb_single(tweets, test_tweets, stemmer=stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.) Let's try using these tricks together, i.e. combining stop words removal with stemming. Do you expect using several tricks together to always work better than using one of them alone? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word removal + stemming\n",
    "lib.evaluate_nb_single(tweets, test_tweets, stop_words=eng_stopwords, stemmer=stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Using N-grams and More Data to Improve Our Naive Bayes Classifier\n",
    "\n",
    "Let's see if we can improve our score by incorporating bigrams and training over more data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using N-grams \n",
    "\n",
    "Currently, `tweet` has an attribute called `tweet.tokenList` which is a list of tokens. You want to add a new attribute to tweet called `tweet.bigramList` which is a list of bigrams.\n",
    "\n",
    "Each bigram should be a pair of strings. You can define the bigram like this: `bigram = (token1, token2)`. In Python, this is called a tuple. You can read more about tuples here: https://www.programiz.com/python-programming/tuple.\n",
    "\n",
    "1.) Fill out the code below to add bigrams to the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bigrams(tweet):\n",
    "    ##### YOUR CODE STARTS HERE ####\n",
    "    tweet.bigramList = [(tweet.tokenList[i], tweet.tokenList[i+1]) for i in range(len(tweet.tokenList)-1)]\n",
    "    ##### YOUR CODE ENDS HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to check if your bigrams are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_tweets, sandy_test_tweets = lib.read_data()\n",
    "haiti_tweets, haiti_test_tweets = lib.read_haiti_data(train_path='../data/pd_haiti_train.csv', \n",
    "                                                      test_path='../data/pd_haiti_test.csv')\n",
    "\n",
    "for tweet in sandy_tweets+sandy_test_tweets+haiti_tweets+haiti_test_tweets:\n",
    "    add_bigrams(tweet)\n",
    "print(\"Checking if bigrams are correct...\")\n",
    "for tweet in sandy_tweets+sandy_test_tweets+haiti_tweets+haiti_test_tweets:\n",
    "    assert tweet._bigramList==tweet.bigramList, \"Error in your implementation of the bigram list!\"\n",
    "print(\"Bigrams are correct.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Re-run the classifier (after adding bigrams to the tweets with the cell above) and get the evaluation score. Did this improve the Naive Bayes classifier score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, test_tweets = sandy_tweets, sandy_test_tweets\n",
    "prior_probs, token_probs = lib.learn_nb(tweets)\n",
    "predictions = [(tweet, lib.classify_nb(tweet, prior_probs, token_probs)) for tweet in test_tweets]\n",
    "labels, precisions, recalls, f1s = lib.evaluate(predictions, has_return=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using historical data\n",
    "A lot of the times, the most effective way to improve a machine learning system is to get more data, which can often be difficult or even infeasible.\n",
    "\n",
    "Fortunately for us though, since Hurricane Sandy happens two years later than the Haiti earthquake, we can use the data from the Haiti earthquake to improve the performance on Hurricane Sandy. Let's try it out!\n",
    "\n",
    "3.) Run the cell below which augments the Sandy training data with all of the Haiti tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandy alone\n",
    "sandy_tweets, sandy_test_tweets = lib.read_data()\n",
    "tweets = sandy_tweets\n",
    "test_tweets = sandy_test_tweets\n",
    "_, _, _, sandy_f1s = lib.evaluate_nb_single(tweets, test_tweets, has_return=True)\n",
    "print()\n",
    "\n",
    "# Augment the training set with historical data from Haiti\n",
    "tweets = haiti_tweets + haiti_test_tweets + sandy_tweets\n",
    "test_tweets = sandy_test_tweets\n",
    "_, _, _, combined_f1s = lib.evaluate_nb_single(tweets, test_tweets, has_return=True)\n",
    "print()\n",
    "\n",
    "print(\"Comparing the performance in the two cases:\")\n",
    "labels = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
    "acc_f1_alone = 0\n",
    "acc_f1_combined = 0\n",
    "for (label, alone, combined) in zip(labels, sandy_f1s, combined_f1s):\n",
    "    print('    {}: alone:{} / combined:{}'.format(label, alone, combined))\n",
    "    acc_f1_alone += alone\n",
    "    acc_f1_combined += combined\n",
    "print('Avg: alone:{} / combined:{}'.format(acc_f1_alone/5, acc_f1_combined/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does adding more data help with the performance? If not, why might this happen?\n",
    "\n",
    "4.) Let's try another way of adding in historical data. Read through the code in the cell below and try to figure out what's changed and why we might expect this to help. Run the cell when you are ready.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandy alone\n",
    "sandy_tweets, sandy_test_tweets = lib.read_data()\n",
    "tweets = sandy_tweets\n",
    "test_tweets = sandy_test_tweets\n",
    "\n",
    "_, _, _, sandy_f1s = lib.evaluate_nb_single(tweets, test_tweets, has_return=True)\n",
    "print()\n",
    "\n",
    "# Augment the training set with historical data from Haiti\n",
    "import random\n",
    "\n",
    "all_haiti_tweets = haiti_tweets + haiti_test_tweets\n",
    "sandy_count = Counter([tweet.category for tweet in sandy_tweets])\n",
    "haiti_count = Counter([tweet.category for tweet in all_haiti_tweets])\n",
    "\n",
    "sample_size = len(all_haiti_tweets)\n",
    "sandy_priors = Counter()\n",
    "for category in sandy_count:\n",
    "    sandy_priors[category] = sandy_count[category] / len(sandy_tweets)\n",
    "    sample_size = min(sample_size, haiti_count[category] / sandy_priors[category])\n",
    "sample_size = int(sample_size)\n",
    "\n",
    "print(\"********************************\")\n",
    "print(\"Adding {:d} examples from Haiti\".format(sample_size))\n",
    "print(\"********************************\\n\")\n",
    "    \n",
    "\n",
    "random.seed(7)\n",
    "haiti_sampled = []\n",
    "for category in haiti_count:\n",
    "    haiti_examples = [tweet for tweet in all_haiti_tweets if tweet.category == category]\n",
    "    haiti_sampled += random.sample(haiti_examples, int(sample_size * sandy_priors[category]))\n",
    "    \n",
    "tweets = sandy_tweets + haiti_sampled\n",
    "test_tweets = sandy_test_tweets\n",
    "\n",
    "_, _, _, combined_f1s = lib.evaluate_nb_single(tweets, test_tweets, has_return=True)\n",
    "print()\n",
    "\n",
    "print(\"Comparing the performance in the two cases:\")\n",
    "labels = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
    "acc_f1_alone = 0\n",
    "acc_f1_combined = 0\n",
    "for (label, alone, combined) in zip(labels, sandy_f1s, combined_f1s):\n",
    "    print('    {}: alone:{} / combined:{}'.format(label, alone, combined))\n",
    "    acc_f1_alone += alone\n",
    "    acc_f1_combined += combined\n",
    "print('Avg: alone:{} / combined:{}'.format(acc_f1_alone/5, acc_f1_combined/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following the category distribution in Sandy when augmenting data, we were able to get gains with historical data! Let's try combining bigrams and historical data. Do you expect to the combination of techniques to help even more? \n",
    "\n",
    "5.) Run the cell below to see if combining techniques helps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandy alone\n",
    "sandy_tweets, sandy_test_tweets = lib.read_data()\n",
    "tweets = sandy_tweets\n",
    "test_tweets = sandy_test_tweets\n",
    "_, _, _, sandy_f1s = lib.evaluate_nb_single(tweets, test_tweets, has_return=True)\n",
    "print()\n",
    "\n",
    "# Augment the training set with historical data from Haiti\n",
    "# following the category distribution in Sandy\n",
    "\n",
    "import random\n",
    "\n",
    "all_haiti_tweets = haiti_tweets + haiti_test_tweets\n",
    "\n",
    "\n",
    "sandy_count = Counter([tweet.category for tweet in sandy_tweets])\n",
    "haiti_count = Counter([tweet.category for tweet in all_haiti_tweets])\n",
    "\n",
    "sample_size = len(all_haiti_tweets)\n",
    "sandy_priors = Counter()\n",
    "for category in sandy_count:\n",
    "    sandy_priors[category] = sandy_count[category] / len(sandy_tweets)\n",
    "    sample_size = min(sample_size, haiti_count[category] / sandy_priors[category])\n",
    "sample_size = int(sample_size)\n",
    "\n",
    "print(\"********************************\")\n",
    "print(\"Adding {:d} examples from Haiti\".format(sample_size))\n",
    "print(\"********************************\\n\")\n",
    "    \n",
    "\n",
    "random.seed(7)\n",
    "haiti_sampled = []\n",
    "for category in haiti_count:\n",
    "    haiti_examples = [tweet for tweet in all_haiti_tweets if tweet.category == category]\n",
    "    haiti_sampled += random.sample(haiti_examples, int(sample_size * sandy_priors[category]))\n",
    "    \n",
    "tweets = sandy_tweets + haiti_sampled\n",
    "test_tweets = sandy_test_tweets\n",
    "prior_probs, token_probs = lib.learn_nb(tweets)\n",
    "predictions = [(tweet, lib.classify_nb(tweet, prior_probs, token_probs)) for tweet in test_tweets]\n",
    "_, _, _, combined_f1s = lib.evaluate(predictions, has_return=True)\n",
    "print()\n",
    "\n",
    "print(\"Comparing the performance in the two cases:\")\n",
    "labels = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
    "acc_f1_alone = 0\n",
    "acc_f1_combined = 0\n",
    "for (label, alone, combined) in zip(labels, sandy_f1s, combined_f1s):\n",
    "    print('    {}: alone:{} / combined:{}'.format(label, alone, combined))\n",
    "    acc_f1_alone += alone\n",
    "    acc_f1_combined += combined\n",
    "print('Avg: alone:{} / combined:{}'.format(acc_f1_alone/5, acc_f1_combined/5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
